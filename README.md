**Maze AI Game** — простая игра с обучением модели, в которой агент учится находить путь в лабиринте, избегая стен и получая штрафы за каждый шаг. Обучение реализовано с помощью алгоритма Q-Learning.

## Необходимые библиотеки

```
pip install pygame
```

### Запуск

```
python agent_007.py
```

### Цель игры

Цель агента — найти кратчайший путь от стартовой позиции до выхода, избегая стен и штрафных шагов.

### Лабиринт

Лабиринт представляет собой матрицу `20x20`, где:
- `0` — проходимые клетки.
- `1` — стены.
- **Стартовая позиция (S)** — нижний левый угол `(19, 0)`.
- **Выход (E)** — верхний правый угол `(0, 19)`.

### Основной алгоритм

Агент обучается с использованием алгоритма Q-Learning. На каждом шаге алгоритм:
1. Выбирает действие (`вверх`, `вниз`, `влево`, `вправо`) на основе **ε-greedy** стратегии.
2. Получает награду:
   - **+100** — достижение выхода.
   - **-1** — каждый шаг.
3. Обновляет **Q-таблицу**, запоминая полезные действия для каждого состояния.

### Стратегия ε-greedy

- С вероятностью **ε** агент выбирает случайное действие, чтобы исследовать лабиринт.
- С вероятностью **1 - ε** выбирается действие с наибольшим значением из Q-таблицы для текущего состояния.
- **ε** постепенно снижается в процессе обучения, чтобы агент полагался на опыт.

### Параметры обучения

- **learning_rate (α)** — скорость обучения, определяет, насколько сильно обновляется Q-таблица.
- **epsilon** — вероятность выбора случайного действия; уменьшается со временем для повышения точности.
- **discount_factor (γ)** — дисконтирующий фактор, определяет важность будущих наград.

## Используемые Формулы

### 1. Формула обновления Q-значения

В алгоритме Q-Learning агент учится на основе обновления значений Q-таблицы. Q-значение для каждого состояния и действия обновляется с учетом текущей награды и ожидаемой будущей награды. Формула обновления Q-значения:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \cdot \left( r + \gamma \cdot \max_{a'} Q(s', a') - Q(s, a) \right)
$$

#### Объяснение переменных:

- **$Q(s, a)$**: Q-значение для текущего состояния $s$ и выбранного действия $a$. Оно представляет собой оценку полезности выбора этого действия в данном состоянии.
- **$\alpha$** (learning rate): Коэффициент скорости обучения, который контролирует, насколько сильно обновляется Q-значение. Значения $\alpha$ находятся в диапазоне от 0 до 1:
  - Если $\alpha$ ближе к 1, Q-значение обновляется быстрее, что может сделать процесс обучения нестабильным.
  - Если $\alpha$ ближе к 0, Q-значение обновляется медленно, что делает обучение более плавным, но медленным.
- **$r$** (reward): Награда за выполнение действия $a$ в состоянии $s$. Она может быть положительной (например, достижение выхода), отрицательной (например, за каждый шаг) или нулевой.
- **$\gamma$** (discount factor): Дисконтирующий фактор, определяющий значимость будущих наград. Значения $\gamma$ также находятся в диапазоне от 0 до 1:
  - Если $\gamma$ ближе к 1, модель будет учитывать будущие награды, что полезно для длинных последовательностей действий.
  - Если $\gamma$ ближе к 0, модель будет фокусироваться только на текущей награде, игнорируя будущее.
- **$\max_{a'} Q(s', a')$**: Максимальное Q-значение для следующего состояния $s'$ и всех возможных действий $a'$. Это значение показывает, каковы лучшие ожидаемые награды в следующем состоянии и используется для оценки долгосрочной полезности текущего действия.
- **$Q(s, a) \leftarrow$**: Означает, что новое Q-значение для текущего состояния и действия вычисляется и обновляется на основе формулы.

#### Пример использования:

Эта формула позволяет агенту корректировать свои ожидания относительно действий на основе опыта. Если действие привело к положительной награде или улучшению состояния, то Q-значение увеличивается, что делает это действие более предпочтительным в будущем.

---

### 2. TD Error (Ошибка временной разницы)

**TD Error** — это промежуточный параметр, который используется для вычисления обновления Q-значения. Он показывает разницу между ожидаемой наградой и текущим Q-значением.

Формула:

$$
TD\ Error = \left( r + \gamma \cdot \max_{a'} Q(s', a') \right) - Q(s, a)
$$

#### Объяснение переменных:

- **$TD\ Error$**: Ошибка временной разницы, показывающая, насколько текущая оценка Q отклоняется от ожидаемого значения.
- **$r$**: Награда за выполненное действие $a$.
- **$\gamma \cdot \max_{a'} Q(s', a')$**: Ожидаемая будущая награда в следующем состоянии $s'$, умноженная на дисконтирующий фактор $\gamma$.
- **$Q(s, a)$**: Текущее Q-значение для состояния $s$ и действия $a$.

#### Пример использования:

TD Error позволяет измерить, насколько текущая оценка полезности состояния отличается от того, что мы могли бы ожидать. Большие значения ошибки указывают на необходимость корректировки Q-значения, чтобы сделать его более точным.

---

### 3. Стратегия выбора действия ε-greedy

Эта стратегия позволяет агенту находить баланс между изучением новых действий (exploration) и использованием уже изученных (exploitation). Агент действует случайным образом с вероятностью **$\epsilon$** и выбирает лучшее действие с вероятностью **$1 - \epsilon$**.

#### Алгоритм:

1. С вероятностью $\epsilon$ агент выбирает случайное действие. Это необходимо для исследования окружающей среды и поиска новых путей.
2. С вероятностью $1 - \epsilon$ агент выбирает действие с наивысшим Q-значением для текущего состояния. Это действие наилучшим образом соответствует накопленному опыту.

#### Параметры:

- **$\epsilon$**: Вероятность выбора случайного действия. Поначалу $\epsilon$ высоко, чтобы агент больше экспериментировал, но по мере обучения оно уменьшается, чтобы сосредоточиться на наилучших действиях.
- **ε decay**: Коэффициент, на который $\epsilon$ уменьшается после каждого эпизода обучения, что постепенно переключает агент на использование накопленного опыта.

#### Пример:

Если $\epsilon = 0.1$, то агент с вероятностью 10% выберет случайное действие, а с вероятностью 90% — наилучшее известное действие. Эта стратегия помогает агенту исследовать лабиринт и одновременно использовать уже накопленный опыт.

Эта ошибка используется для корректировки Q-значения, минимизируя разницу между ожидаемой и фактической наградой.

## Основные функции

- **choose_action(state)** — выбирает действие на основе ε-greedy стратегии.
- **update_q_table(state, action, reward, next_state)** — обновляет значения Q-таблицы.
- **draw_maze(agent_position, episode, steps)** — визуализирует положение агента в лабиринте.

### Функция выбора действия

```
def choose_action(state):
    global epsilon
    if random.uniform(0, 1) < epsilon:
        return random.randint(0, 3)
    return np.argmax(q_table[state[0], state[1]])
```

### Функция обновления Q-таблицы

```
def update_q_table(state, action, reward, next_state):
    best_next_action = np.argmax(q_table[next_state[0], next_state[1]])
    td_target = reward + discount_factor * q_table[next_state[0], next_state[1], best_next_action]
    td_error = td_target - q_table[state[0], state[1], action]
    q_table[state[0], state[1], action] += learning_rate * td_error
```

### Функция визуализации

```
def draw_maze(agent_position, episode, steps):
    screen.fill(colors["path"])
    for y in range(height):
        for x in range(width):
            if (y, x) == start:
                pygame.draw.rect(screen, colors["start"], pygame.Rect(x * cell_size, y * cell_size, cell_size, cell_size))
            elif (y, x) == end:
                pygame.draw.rect(screen, colors["end"], pygame.Rect(x * cell_size, y * cell_size, cell_size, cell_size))
            elif maze[y, x] == 1:
                pygame.draw.rect(screen, colors["wall"], pygame.Rect(x * cell_size, y * cell_size, cell_size, cell_size))
    pygame.draw.rect(screen, colors["agent"], pygame.Rect(agent_position[1] * cell_size, agent_position[0] * cell_size, cell_size, cell_size))
    font = pygame.font.Font(None, 36)
    text = font.render(f"Episode: {episode}  Steps: {steps}", True, (0, 0, 0))
    screen.blit(text, (125, height * cell_size - 20))
    pygame.display.flip()
```




